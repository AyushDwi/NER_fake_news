# Import necessary libraries
import pandas as pd
import re
import spacy
from textblob import TextBlob
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.model_selection import StratifiedKFold, cross_val_score, cross_val_predict
from sklearn.metrics import mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Load spaCy model
nlp = spacy.load("en_core_web_sm")

# ================================
# 1. Text Preprocessing
# ================================
def preprocess_text(text):
    # Remove HTML tags
    text = re.sub(r"<.*?>", "", text)
    # Remove special characters and unnecessary whitespaces
    text = re.sub(r"[^a-zA-Z0-9\s]", "", text).strip()
    # Normalize text to lowercase
    text = text.lower()
    # Tokenize and remove stopwords using spaCy
    doc = nlp(text)
    tokens = [token.text for token in doc if not token.is_stop and token.is_alpha]
    return " ".join(tokens)

# ================================
# 2. Named Entity Recognition (NER)
# ================================
def extract_ner_features(text):
    doc = nlp(text)
    org_count = len([ent for ent in doc.ents if ent.label_ == "ORG"])
    gpe_count = len([ent for ent in doc.ents if ent.label_ == "GPE"])
    person_count = len([ent for ent in doc.ents if ent.label_ == "PERSON"])
    return pd.Series([org_count, gpe_count, person_count], index=["org_count", "gpe_count", "person_count"])

# ================================
# 3. Load and Merge Datasets
# ================================
urls = {
    "politifact_fake": "https://raw.githubusercontent.com/KaiDMML/FakeNewsNet/master/dataset/politifact_fake.csv",
    "politifact_real": "https://raw.githubusercontent.com/KaiDMML/FakeNewsNet/master/dataset/politifact_real.csv",
    "gossipcop_fake": "https://raw.githubusercontent.com/KaiDMML/FakeNewsNet/master/dataset/gossipcop_fake.csv",
    "gossipcop_real": "https://raw.githubusercontent.com/KaiDMML/FakeNewsNet/master/dataset/gossipcop_real.csv"
}

politifact_fake = pd.read_csv(urls["politifact_fake"])
politifact_real = pd.read_csv(urls["politifact_real"])
gossipcop_fake = pd.read_csv(urls["gossipcop_fake"])
gossipcop_real = pd.read_csv(urls["gossipcop_real"])

# Add labels and combine datasets
politifact_fake['label'] = 0
politifact_real['label'] = 1
gossipcop_fake['label'] = 0
gossipcop_real['label'] = 1

df = pd.concat([politifact_fake, politifact_real, gossipcop_fake, gossipcop_real], ignore_index=True)

# Use 'title' as the text column
text_column = "title"
df[text_column] = df[text_column].fillna("").astype(str)

# ================================
# 4. Preprocessing and NER
# ================================
df['cleaned_text'] = df[text_column].apply(preprocess_text)
df[['org_count', 'gpe_count', 'person_count']] = df['cleaned_text'].apply(lambda x: extract_ner_features(x))

# ================================
# 5. Feature Engineering
# ================================
# Article length
df['article_length'] = df['cleaned_text'].apply(lambda x: len(x.split()))

# Sentiment scores
df['sentiment_score'] = df['cleaned_text'].apply(lambda x: TextBlob(x).sentiment.polarity)

# Engagement metric (use tweet_ids length as a proxy for engagement)
df['engagement_metric'] = df['tweet_ids'].apply(lambda x: len(str(x).split(",")))

# ================================
# 6. Model Training and Evaluation
# ================================
# Define features and target
features = ["org_count", "gpe_count", "person_count", "article_length", "sentiment_score"]
X = df[features].values
y_class = df['label'].values  # Classification target
y_regress = df['engagement_metric'].values  # Regression target

# Initialize 10-fold CV
cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

# Random Forest for Classification
clf = RandomForestClassifier(random_state=42)
accuracy = cross_val_score(clf, X, y_class, cv=cv, scoring="accuracy").mean()
f1 = cross_val_score(clf, X, y_class, cv=cv, scoring="f1").mean()

# Random Forest for Regression
reg = RandomForestRegressor(random_state=42)
y_reg_pred = cross_val_predict(reg, X, y_regress, cv=cv)
r_squared = r2_score(y_regress, y_reg_pred)
mae = mean_absolute_error(y_regress, y_reg_pred)

# Display results
print(f"10-Fold CV Accuracy (Classification): {accuracy:.2f}")
print(f"10-Fold CV F1 Score (Classification): {f1:.2f}")
print(f"10-Fold CV RÂ² Score (Regression): {r_squared:.2f}")
print(f"10-Fold CV Mean Absolute Error (Regression): {mae:.2f}")

# ================================
# 7. Visualization
# ================================
# Bar chart for entity frequencies
entity_counts = df[['org_count', 'gpe_count', 'person_count']].sum()
entity_counts.plot(kind="bar", color=["blue", "orange", "green"], title="Entity Frequency in Articles")
plt.xlabel("Entity Type")
plt.ylabel("Frequency")
plt.show()

# Scatter plot for sentiment vs engagement
sns.scatterplot(x=df['sentiment_score'], y=df['engagement_metric'])
plt.title("Sentiment vs Engagement")
plt.xlabel("Sentiment Score")
plt.ylabel("Engagement Metric")
plt.show()

# Heatmap for feature correlations
correlation_matrix = df[features + ['engagement_metric']].corr()
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm")
plt.title("Feature Correlation Heatmap")
plt.show()
